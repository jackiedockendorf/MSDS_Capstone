{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pprint\n",
    "import warnings\n",
    "import string \n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "#nltk imports \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "from nltk.util import ngrams \n",
    "\n",
    "#sklean imports \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#spaCy imports \n",
    "import spacy\n",
    "nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
    "\n",
    "# plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Gensim imports \n",
    "import gensim\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Set up log to terminal for Gensim \n",
    "import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#pyLDAvis import \n",
    "import pyLDAvis.gensim\n",
    "\n",
    "#For progress bar to monitor status of things running \n",
    "from tqdm import tqdm\n",
    "\n",
    "#Load contraction map python file \n",
    "from contractions import CONTRACTION_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_df = pd.read_csv(\"essays_export.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = pd.Series(' '.join(essay_df['ESSAY']).lower().split()).value_counts()[:500]\n",
    "top_words\n",
    "\n",
    "wordsFiltered = {}\n",
    "for key, value in top_words.items():\n",
    "    if key not in stopword_list:\n",
    "        wordsFiltered.setdefault(key, value)\n",
    "\n",
    "#wordsFiltered\n",
    "#top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncleaned Data Top Tri-grams \n",
    "\n",
    "def get_top_ngram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) \n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]\n",
    "\n",
    "top_tri_grams=get_top_ngram(essay_df['ESSAY'],n=3)\n",
    "x,y=map(list,zip(*top_tri_grams))\n",
    "sns.set_context(\"talk\")\n",
    "sns.barplot(x=y,y=x, palette=\"muted\").set_title('Top Trigrams from Essay Data',fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def normalize_text(text): \n",
    "    text = expand_contractions(text)\n",
    "    #lowercase  \n",
    "    text = text.lower()\n",
    "    #remove new line, carriage return, and tab characters \n",
    "    text = text.replace('\\n', ' ').replace('\\r', '').replace('\\t', '')\n",
    "    #remove unwanted characters \n",
    "    text = text.replace('Â¿', '')\n",
    "    #remove accented characters \n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    #remove punctuation \n",
    "    for p in string.punctuation: \n",
    "        text = text.replace(p, \"\")\n",
    "    #remove stop words \n",
    "    text = remove_stopwords(text, is_lower_case=True)\n",
    "    #lemmatize \n",
    "    text = lemmatize_text(text)\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_df['ESSAY'] = essay_df['ESSAY'].apply(normalize_text)\n",
    "#essay_df['CLEAN_ESSAY'] = essay_df['ESSAY'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned Data Top Tri-grams \n",
    "\n",
    "top_tri_grams=get_top_ngram(essay_df['ESSAY'],n=3)\n",
    "x,y=map(list,zip(*top_tri_grams))\n",
    "sns.set_context(\"talk\")\n",
    "sns.barplot(x=y,y=x).set_title('Top Trigrams from Essay Data',fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_grams=get_top_ngram(essay_df['ESSAY'],n=1)\n",
    "x,y=map(list,zip(*top_n_grams))\n",
    "#plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "plt.style.use('tableau-colorblind10')\n",
    "ax.barh(x, y, align='center')\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Count')\n",
    "ax.yaxis.set_tick_params(labelsize='large')\n",
    "ax.set_title('Top Words in Essay Data', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.savefig(\"Top_Essay_Words.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = essay_df.ESSAY.tolist()\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Gensim Dictionary and Filter Extremes (based on word frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial LDA Model (Gensim Standard Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = lda_model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_initial = {'Type': 'Initial LDA', 'Coherence Score': coherence_lda, 'Number of Topics': 10}\n",
    "initial_models_df = pd.DataFrame(lda_initial, index=[0])\n",
    "initial_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lda_vis(lda_model, bow_corpus, dic):\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)\n",
    "    return vis \n",
    "\n",
    "plot_lda_vis(lda_model, corpus, dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def wordcloud_topics_gensim(model, no_top_words=40):\n",
    "    for topic in range(0, model.num_topics):\n",
    "        size = {}\n",
    "        for (word, prob) in model.show_topic(topic, topn=no_top_words):\n",
    "            size[word] = prob\n",
    "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "        wc.generate_from_frequencies(size)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        # if you don't want to save the topic model, comment the next line\n",
    "        plt.savefig(f'{model}_topic{topic}.png')\n",
    "        \n",
    "wordcloud_topics_gensim(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lda_topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in lda_model.show_topic(n, topn=20)] \n",
    "                   for n in range(0, lda_model.num_topics)]\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "initial_lda_topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in initial_lda_topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, lda_model.num_topics+1)]\n",
    "                         )\n",
    "#initial_lda_topics_df\n",
    "\n",
    "initial_lda_topics_df['Label'] = ['Random', 'Community Health', 'Engineering / Biomedical', 'Disease / Illness', 'Humans / Culture', 'Camp/Childhood', 'Med Experiences', 'Academics', 'Random', 'Community Service']\n",
    "initial_lda_topics_df = initial_lda_topics_df.style.set_properties(**{'text-align': 'left'})\n",
    "\n",
    "initial_lda_topics_df.set_properties(subset=['Terms per Topic'], **{'width': '500px'})\n",
    "\n",
    "initial_lda_topics_df = initial_lda_topics_df.set_table_styles(\n",
    "[dict(selector = 'th', props=[('text-align', 'left')])])\n",
    "\n",
    "initial_lda_topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Non Negative Matrix Factorization Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non Negative Matrix Factorization Topic Modeling \n",
    "nmf_gensim = Nmf(corpus, num_topics=10, id2word=id2word, passes=passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = nmf_gensim.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CoherenceModel(model=nmf_gensim, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "cv_nnmf = cv.get_coherence()\n",
    "print('\\nCoherence Score: ', cv_nnmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_initial = {'Type': 'Initial NMF', 'Coherence Score': cv_nnmf, 'Number of Topics': 10}\n",
    "initial_models_df = initial_models_df.append(nmf_initial, ignore_index = True)\n",
    "initial_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_topics_gensim(nmf_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_nmf_topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in nmf_gensim.show_topic(n, topn=20)] \n",
    "                   for n in range(0, nmf_gensim.num_topics)]\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "initial_nmf_topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in initial_nmf_topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, nmf_gensim.num_topics+1)]\n",
    "                         )\n",
    "\n",
    "#initial_nmf_topics_df['Label'] = ['Random', 'Community Health', 'Engineering / Biomedical', 'Disease / Illness', 'Humans / Culture', 'Camp/Childhood', 'Med Experiences', 'Academics', 'Random', 'Community Service']\n",
    "initial_nmf_topics_df = initial_nmf_topics_df.style.set_properties(**{'text-align': 'left'})\n",
    "\n",
    "initial_nmf_topics_df.set_properties(subset=['Terms per Topic'], **{'width': '500px'})\n",
    "\n",
    "initial_nmf_topics_df = initial_nmf_topics_df.set_table_styles(\n",
    "[dict(selector = 'th', props=[('text-align', 'left')])])\n",
    "\n",
    "initial_nmf_topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial MALLET-LDA Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = 'mallet-2.0.8/bin/mallet' \n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CoherenceModel(model=ldamallet, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "cv_mallet = cv.get_coherence()\n",
    "print('\\nCoherence Score: ', cv_mallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_initial = {'Type': 'Initial MALLET-LDA', 'Coherence Score': cv_mallet, 'Number of Topics': 10}\n",
    "initial_models_df = initial_models_df.append(mallet_initial, ignore_index = True)\n",
    "initial_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_topics_gensim(ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_lda_model=gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(mallet_lda_model, corpus, dictionary, sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = mallet_lda_model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_mallet_topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in mallet_lda_model.show_topic(n, topn=20)] \n",
    "                   for n in range(0, mallet_lda_model.num_topics)]\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "initial_mallet_topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in initial_mallet_topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, mallet_lda_model.num_topics+1)]\n",
    "                         )\n",
    "\n",
    "#initial_nmf_topics_df['Label'] = ['Random', 'Community Health', 'Engineering / Biomedical', 'Disease / Illness', 'Humans / Culture', 'Camp/Childhood', 'Med Experiences', 'Academics', 'Random', 'Community Service']\n",
    "initial_mallet_topics_df = initial_mallet_topics_df.style.set_properties(**{'text-align': 'left'})\n",
    "\n",
    "initial_mallet_topics_df.set_properties(subset=['Terms per Topic'], **{'width': '500px'})\n",
    "\n",
    "initial_mallet_topics_df = initial_mallet_topics_df.set_table_styles(\n",
    "[dict(selector = 'th', props=[('text-align', 'left')])])\n",
    "\n",
    "initial_mallet_topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Tuning for Number of Topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def lda_coherence_generator(corpus, id2word, chunksize=2000, texts=docs, alpha='auto', eta='auto', iterations=400, passes=20, eval_every=None, \n",
    "                                    start_topic_count=5, end_topic_count=15, step=1):\n",
    "    \n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):\n",
    "        model = LdaModel(corpus=corpus, \n",
    "                         id2word=id2word, \n",
    "                         chunksize=chunksize, \n",
    "                         alpha=alpha,\n",
    "                         eta=eta,\n",
    "                         iterations=iterations,\n",
    "                         num_topics=topic_nums, \n",
    "                         passes=passes,\n",
    "                         eval_every=eval_every)\n",
    "        \n",
    "        coherence_model_lda = CoherenceModel(model=model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        #top_topics = model.top_topics(corpus)\n",
    "        # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "       # avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "        coherence_scores.append(coherence_lda)\n",
    "        models.append(model)\n",
    "    \n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_models, coherence_scores_lda = lda_coherence_generator(corpus=corpus, id2word=id2word, start_topic_count=5, end_topic_count=30, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_coherence_df = pd.DataFrame({'Number of Topics': range(5, 31, 1),\n",
    "                             'Coherence Score': np.round(coherence_scores_lda, 4)})\n",
    "lda_coherence_df = lda_coherence_df.sort_values(by=['Coherence Score'], ascending=False).head(26)\n",
    "lda_coherence_df.style.set_caption(\"LDA Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "x_ax = range(5, 31, 1)\n",
    "y_ax = coherence_scores_lda\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_ax, c='r')\n",
    "plt.ylim(.25, .35)\n",
    "#plt.axhline(y=0.535, c='k', linestyle='--', linewidth=2)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "xl = plt.xlabel('Number of Topics')\n",
    "yl = plt.ylabel('Coherence Score')\n",
    "plt.title(\"LDA Coherence Scores as a Function of Number of Topics\", fontweight=\"bold\")\n",
    "plt.savefig(\"LDA_Graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_lda_index = lda_coherence_df.index[lda_coherence_df['Coherence Score'].idxmax()]\n",
    "print(\"The Best LDA model is:\")\n",
    "\n",
    "best_lda_index_list = lda_coherence_df['Coherence Score'].nlargest(1).index.tolist()\n",
    "index_list = [str(integer) for integer in best_lda_index_list]\n",
    "index_str = \"\".join(index_list)\n",
    "best_lda_index = int(index_str)\n",
    "\n",
    "lda_coherence_df.loc[best_lda_index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save best model details \n",
    "lda_model_best = lda_coherence_df.loc[best_lda_index]\n",
    "best_lda = lda_models[best_lda_index]\n",
    "lda_best = {'Type': 'LDA', 'Coherence Score': lda_model_best['Coherence Score'], 'Number of Topics': lda_model_best['Number of Topics'], 'Name of Model': 'lda_model_best'}\n",
    "all_best_models_df = pd.DataFrame(lda_best, index=[0])\n",
    "\n",
    "# Show best LDA model's topics \n",
    "def show_me_topics(model, corpus, num_topics):\n",
    "    top_topics = model.top_topics(corpus)\n",
    "    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "    print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "    pprint(top_topics)\n",
    "\n",
    "#Show best model's topics \n",
    "show_me_topics(best_lda, corpus, num_topics=lda_model_best['Number of Topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lda_vis(best_lda, corpus, dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda_topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in best_lda.show_topic(n, topn=20)] \n",
    "                   for n in range(0, best_lda.num_topics)]\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "best_lda_topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in best_lda_topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, best_lda.num_topics+1)]\n",
    "                         )\n",
    "#initial_lda_topics_df\n",
    "#best_lda_topics_df['Label'] = ['Random', 'Community Health', 'Engineering / Biomedical', 'Disease / Illness', 'Humans / Culture', 'Camp/Childhood', 'Med Experiences', 'Academics', 'Random', 'Community Service']\n",
    "best_lda_topics_df = best_lda_topics_df.style.set_properties(**{'text-align': 'left'})\n",
    "best_lda_topics_df.set_properties(subset=['Terms per Topic'], **{'width': '500px'})\n",
    "best_lda_topics_df = best_lda_topics_df.set_table_styles(\n",
    "[dict(selector = 'th', props=[('text-align', 'left')])])\n",
    "best_lda_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle top LDA Model \n",
    "\n",
    "filename = 'best_LDA_model_april_27_2021.sav'\n",
    "#pickle.dump(best_lda, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnmf_coherence_generator(corpus, id2word, chunksize=2000, texts=docs, alpha='auto', eta='auto', iterations=400, passes=20, eval_every=None, \n",
    "                                    start_topic_count=5, end_topic_count=15, step=1):\n",
    "    \n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):\n",
    "        model = Nmf(corpus, num_topics=topic_nums, id2word=id2word, passes=passes)\n",
    "        cv = CoherenceModel(model=model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "        cv_nnmf = cv.get_coherence()\n",
    "        coherence_scores.append(cv_nnmf)\n",
    "        models.append(model)\n",
    "    \n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_models, coherence_scores_nmf = nnmf_coherence_generator(corpus=corpus, id2word=id2word, start_topic_count=5, end_topic_count=30, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_coherence_df = pd.DataFrame({'Number of Topics': range(5, 31, 1),\n",
    "                             'Coherence Score': np.round(coherence_scores_nmf, 4)})\n",
    "nmf_coherence_df.sort_values(by=['Coherence Score'], ascending=False).head(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "x_ax = range(5, 31, 1)\n",
    "y_ax = coherence_scores_nmf\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_ax, c='r')\n",
    "plt.ylim(.3, .4)\n",
    "#plt.axhline(y=0.4, c='k', linestyle='--', linewidth=2)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "xl = plt.xlabel('Number of Topics')\n",
    "yl = plt.ylabel('Coherence Score')\n",
    "plt.title(\"NMF Coherence Scores as a Function of Number of Topics\", fontweight=\"bold\")\n",
    "plt.savefig(\"NMF_Graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_nmf_index = nmf_coherence_df.index[nmf_coherence_df['Coherence Score'].idxmax()]\n",
    "#nmf_coherence_df.loc[best_nmf_index]\n",
    "\n",
    "print(\"The Best NMF model is:\")\n",
    "\n",
    "best_nmf_index_list = nmf_coherence_df['Coherence Score'].nlargest(1).index.tolist()\n",
    "nmf_index_list = [str(integer) for integer in best_nmf_index_list]\n",
    "nmf_index_str = \"\".join(nmf_index_list)\n",
    "best_nmf_index = int(nmf_index_str)\n",
    "\n",
    "nmf_coherence_df.loc[best_nmf_index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save best model details \n",
    "nmf_model_best = nmf_coherence_df.loc[best_nmf_index]\n",
    "best_nmf = nmf_models[best_nmf_index]\n",
    "nmf_best = {'Type': 'NMF', 'Coherence Score': nmf_model_best['Coherence Score'], 'Number of Topics': nmf_model_best['Number of Topics'] , 'Name of Model': 'nmf_model_best'}\n",
    "all_best_models_df = all_best_models_df.append(nmf_best, ignore_index = True)\n",
    "\n",
    "#Show best model's topics \n",
    "show_me_topics(best_nmf, corpus, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nmf_topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in best_nmf.show_topic(n, topn=20)] \n",
    "                   for n in range(0, best_nmf.num_topics)]\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "best_nmf_topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in best_nmf_topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, best_nmf.num_topics+1)]\n",
    "                         )\n",
    "#initial_lda_topics_df\n",
    "#best_lda_topics_df['Label'] = ['Random', 'Community Health', 'Engineering / Biomedical', 'Disease / Illness', 'Humans / Culture', 'Camp/Childhood', 'Med Experiences', 'Academics', 'Random', 'Community Service']\n",
    "best_nmf_topics_df = best_nmf_topics_df.style.set_properties(**{'text-align': 'left'})\n",
    "best_nmf_topics_df.set_properties(subset=['Terms per Topic'], **{'width': '500px'})\n",
    "best_nmf_topics_df = best_nmf_topics_df.set_table_styles(\n",
    "[dict(selector = 'th', props=[('text-align', 'left')])])\n",
    "best_nmf_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle top NMF Model \n",
    "\n",
    "filename = 'best_NMF_model_april_27_2021.sav'\n",
    "#pickle.dump(best_nmf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Mallet Topic Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mallet_coherence_generator(corpus, id2word, chunksize=2000, texts=docs, alpha='auto', eta='auto', \n",
    "                               iterations=400, passes=20, eval_every=None, mallet_path='mallet-2.0.8/bin/mallet',\n",
    "                               start_topic_count=5, end_topic_count=15, step=1):\n",
    "    \n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=topic_nums, id2word=id2word)\n",
    "        cv = CoherenceModel(model=ldamallet, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "        cv_mallet = cv.get_coherence()\n",
    "        coherence_scores.append(cv_mallet)\n",
    "        models.append(ldamallet)\n",
    "    \n",
    "    return models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_models, coherence_scores_mallet = mallet_coherence_generator(corpus=corpus, id2word=id2word, start_topic_count=5, end_topic_count=30, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_coherence_df = pd.DataFrame({'Number of Topics': range(5, 31, 1),\n",
    "                             'Coherence Score': np.round(coherence_scores_mallet, 4)})\n",
    "mallet_coherence_df.sort_values(by=['Coherence Score'], ascending=False).head(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "x_ax = range(5, 31, 1)\n",
    "y_ax = coherence_scores_mallet\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_ax, c='r')\n",
    "plt.ylim(.35, .45)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "xl = plt.xlabel('Number of Topics')\n",
    "yl = plt.ylabel('Coherence Score')\n",
    "plt.title(\"MALLET-LDA Coherence Scores as a Function of Number of Topics\", fontweight=\"bold\")\n",
    "plt.savefig(\"MALLET_LDA _Graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find model with highest coherence score \n",
    "#best_mallet_index = mallet_coherence_df.index[mallet_coherence_df['Coherence Score'].idxmax()]\n",
    "#mallet_coherence_df.loc[best_mallet_index]\n",
    "\n",
    "print(\"The Best MALLET-LDA model is:\")\n",
    "best_mallet_index_list = mallet_coherence_df['Coherence Score'].nlargest(1).index.tolist()\n",
    "mallet_index_list = [str(integer) for integer in best_mallet_index_list]\n",
    "mallet_index_str = \"\".join(mallet_index_list)\n",
    "best_mallet_index = int(mallet_index_str)\n",
    "\n",
    "mallet_coherence_df.loc[best_mallet_index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save best model details \n",
    "mallet_model_best = mallet_coherence_df.loc[best_mallet_index]\n",
    "best_mallet = mallet_models[best_mallet_index]\n",
    "mallet_best = {'Type': 'MALLET-LDA', 'Coherence Score': mallet_model_best['Coherence Score'], 'Number of Topics': mallet_model_best['Number of Topics'] , 'Name of Model': 'mallet_model_best'}\n",
    "all_best_models_df = all_best_models_df.append(mallet_best, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to Gensim LDA model \n",
    "best_mallet_lda_model=gensim.models.wrappers.ldamallet.malletmodel2ldamodel(best_mallet)\n",
    "\n",
    "#Show best model's topics \n",
    "show_me_topics(best_mallet_lda_model, corpus, mallet_model_best['Number of Topics'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mallet_topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in best_mallet_lda_model.show_topic(n, topn=20)] \n",
    "                   for n in range(0, best_mallet_lda_model.num_topics)]\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "best_mallet_topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in best_mallet_topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, best_mallet_lda_model.num_topics+1)]\n",
    "                         )\n",
    "#initial_lda_topics_df\n",
    "#best_lda_topics_df['Label'] = ['Random', 'Community Health', 'Engineering / Biomedical', 'Disease / Illness', 'Humans / Culture', 'Camp/Childhood', 'Med Experiences', 'Academics', 'Random', 'Community Service']\n",
    "best_mallet_topics_df = best_mallet_topics_df.style.set_properties(**{'text-align': 'left'})\n",
    "best_mallet_topics_df.set_properties(subset=['Terms per Topic'], **{'width': '500px'})\n",
    "best_mallet_topics_df = best_mallet_topics_df.set_table_styles(\n",
    "[dict(selector = 'th', props=[('text-align', 'left')])])\n",
    "best_mallet_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(best_mallet_lda_model, corpus, dictionary, sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle top MALLET Model \n",
    "best_mallet = mallet_models[21]\n",
    "filename = 'best_MALLET_model_april_27_2021.sav'\n",
    "#pickle.dump(best_mallet_lda_model, open(filename, 'wb'))\n",
    "\n",
    "mallet_choice_2_model = mallet_models[8]\n",
    "file2name = 'MALLET_second_april_27_2021.sav'\n",
    "#pickle.dump(best_mallet_lda_model, open(file2name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Models Comparison and Final Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('tableau-colorblind10')\n",
    "%matplotlib inline\n",
    "\n",
    "x_ax = range(5, 31, 1)\n",
    "y_1 = coherence_scores_mallet\n",
    "y_2 = coherence_scores_nmf\n",
    "y_3 = coherence_scores_lda\n",
    "\n",
    "# plot lines\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_1, label = \"MALLET-LDA\")\n",
    "plt.plot(x_ax, y_2, label = \"NMF\")\n",
    "plt.plot(x_ax, y_3, label = \"LDA\")\n",
    "plt.legend()\n",
    "plt.ylim(.25, .45)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "#ax.yaxis.set_tick_params(labelsize='large')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc=2, prop={'size': 15})\n",
    "xl = plt.xlabel('Number of Topics', fontsize=15)\n",
    "yl = plt.ylabel('Coherence Score', fontsize=15)\n",
    "plt.title(\"Coherence Scores as a Function of Number of Topics\", fontweight=\"bold\", fontsize=20)\n",
    "plt.savefig(\"All_MODELS_Graph2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The top models for each method\")\n",
    "all_best_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_index = all_best_models_df.index[all_best_models_df['Coherence Score'].idxmax()]\n",
    "print(\"The Best Model is:\")\n",
    "all_best_models_df.loc[best_model_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Other Top MALLET Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MALLET-LDA Model with Second Best Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_choice_2_model = mallet_models[23]\n",
    "#convert to Gensim LDA model \n",
    "mallet_second_best_model=gensim.models.wrappers.ldamallet.malletmodel2ldamodel(mallet_choice_2_model)\n",
    "\n",
    "#Show best model's topics \n",
    "show_me_topics(mallet_second_best_model, corpus, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(mallet_second_best_model, corpus, dictionary, sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MALLET-LDA Model with Third Best Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_choice_3_model = mallet_models[22]\n",
    "#convert to Gensim LDA model \n",
    "mallet_3_model=gensim.models.wrappers.ldamallet.malletmodel2ldamodel(mallet_choice_3_model)\n",
    "\n",
    "#Show best model's topics \n",
    "show_me_topics(mallet_3_model, corpus, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(mallet_3_model, corpus, dictionary, sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_2_topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in mallet_second_best_model.show_topic(n, topn=20)] \n",
    "                   for n in range(0, mallet_second_best_model.num_topics)]\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "mallet_2_topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in mallet_2_topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, mallet_second_best_model.num_topics+1)]\n",
    "                         )\n",
    "#initial_lda_topics_df\n",
    "#best_lda_topics_df['Label'] = ['Random', 'Community Health', 'Engineering / Biomedical', 'Disease / Illness', 'Humans / Culture', 'Camp/Childhood', 'Med Experiences', 'Academics', 'Random', 'Community Service']\n",
    "mallet_2_topics_df = mallet_2_topics_df.style.set_properties(**{'text-align': 'left'})\n",
    "mallet_2_topics_df.set_properties(subset=['Terms per Topic'], **{'width': '500px'})\n",
    "mallet_2_topics_df = mallet_2_topics_df.set_table_styles(\n",
    "[dict(selector = 'th', props=[('text-align', 'left')])])\n",
    "mallet_2_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_topics_gensim(mallet_second_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(essay_df.ESSAY)):\n",
    "    top_topics = mallet_second_best_model.get_document_topics(corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(28)]\n",
    "    #topic_vec.extend([essay_df.iloc[i].MED_AAMC_ID]) # removing from capstone deliverable for data privacy reasons\n",
    "    #topic_vec.extend([essay_df.iloc[i].ESSAY]) # removing from capstone deliverable for data privacy reasons\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(train_vecs)\n",
    "feature_df.columns = ['Humanities',\n",
    "'Doctor Relationship',\n",
    "'Summer Experience',\n",
    "'Decision Process',\n",
    "'Service',\n",
    "'Death / Suffering ',\n",
    "'Nursing Home Experience',\n",
    "'Healthcare Access & Equity',\n",
    "'Public Health',\n",
    "'Practice Medicine',\n",
    "'Observe / Shadow',\n",
    "'Anatomy Interest',\n",
    "'Athletics',\n",
    "'Physician Skills',\n",
    "'Impact',\n",
    "'Children / Childhood',\n",
    "'Moments',\n",
    "'Academics',\n",
    "'Culture / Travel',\n",
    "'Emergency Medicine',\n",
    "'Disease / Treatment',\n",
    "'Family Illness',\n",
    "'Rural Community',\n",
    "'Problem Solving',\n",
    "'Become Doctor',\n",
    "'Goals / Tenacity',\n",
    "'Lab Experience',\n",
    "'Biomedical Engineering']\n",
    "#'AAMC_ID', # removing from capstone deliverable for data privacy reasons\n",
    "#'ESSAY' # removing from capstone deliverable for data privacy reasons\n",
    "\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_results = mallet_second_best_model[corpus]\n",
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0] \n",
    "                     for topics in tm_results]\n",
    "\n",
    "feature_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
    "feature_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
    "feature_df['Topic Desc'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
    "conditions = [\n",
    "    (feature_df['Dominant Topic'] == 1),\n",
    "    (feature_df['Dominant Topic'] == 2),\n",
    "    (feature_df['Dominant Topic'] == 3),\n",
    "    (feature_df['Dominant Topic'] == 4),\n",
    "    (feature_df['Dominant Topic'] == 5),\n",
    "    (feature_df['Dominant Topic'] == 6),\n",
    "    (feature_df['Dominant Topic'] == 7),\n",
    "    (feature_df['Dominant Topic'] == 8),\n",
    "    (feature_df['Dominant Topic'] == 9),\n",
    "    (feature_df['Dominant Topic'] == 10),\n",
    "    (feature_df['Dominant Topic'] == 11),\n",
    "    (feature_df['Dominant Topic'] == 12),\n",
    "    (feature_df['Dominant Topic'] == 13),\n",
    "    (feature_df['Dominant Topic'] == 14),\n",
    "    (feature_df['Dominant Topic'] == 15),\n",
    "    (feature_df['Dominant Topic'] == 16),\n",
    "    (feature_df['Dominant Topic'] == 17),\n",
    "    (feature_df['Dominant Topic'] == 18),\n",
    "    (feature_df['Dominant Topic'] == 19),\n",
    "    (feature_df['Dominant Topic'] == 20),\n",
    "    (feature_df['Dominant Topic'] == 21),\n",
    "    (feature_df['Dominant Topic'] == 22),\n",
    "    (feature_df['Dominant Topic'] == 23),\n",
    "    (feature_df['Dominant Topic'] == 24),\n",
    "    (feature_df['Dominant Topic'] == 25),\n",
    "    (feature_df['Dominant Topic'] == 26),\n",
    "    (feature_df['Dominant Topic'] == 27),\n",
    "    (feature_df['Dominant Topic'] == 28),]\n",
    "choices = ['Humanities',\n",
    "'Doctor Relationship',\n",
    "'Summer Experience',\n",
    "'Decision Process',\n",
    "'Service',\n",
    "'Death / Suffering ',\n",
    "'Nursing Home Experience',\n",
    "'Healthcare Access & Equity',\n",
    "'Public Health',\n",
    "'Practice Medicine',\n",
    "'Observe / Shadow',\n",
    "'Anatomy Interest',\n",
    "'Athletics',\n",
    "'Physician Skills',\n",
    "'Impact',\n",
    "'Children / Childhood',\n",
    "'Moments',\n",
    "'Academics',\n",
    "'Culture / Travel',\n",
    "'Emergency Medicine',\n",
    "'Disease / Treatment',\n",
    "'Family Illness',\n",
    "'Rural Community',\n",
    "'Problem Solving',\n",
    "'Become Doctor',\n",
    "'Goals / Tenacity',\n",
    "'Lab Experience',\n",
    "'Biomedical Engineering']\n",
    "feature_df['Dominant Topic Label'] = np.select(conditions, choices)\n",
    "\n",
    "#Hiding Output for data privacy Reasons \n",
    "#feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "topic_stats_df = corpus_topic_df.groupby('Dominant Topic').agg({\n",
    "                                                'Dominant Topic': {\n",
    "                                                    'Doc Count': np.size,\n",
    "                                                    '% Total Docs': np.size }\n",
    "                                              })\n",
    "topic_stats_df = topic_stats_df['Dominant Topic'].reset_index()\n",
    "topic_stats_df['% Total Docs'] = topic_stats_df['% Total Docs'].apply(lambda row: round((row*100) / len(essay_df.ESSAY), 2))\n",
    "conditions = [\n",
    "    (topic_stats_df['Dominant Topic'] == 1),\n",
    "    (topic_stats_df['Dominant Topic'] == 2),\n",
    "    (topic_stats_df['Dominant Topic'] == 3),\n",
    "    (topic_stats_df['Dominant Topic'] == 4),\n",
    "    (topic_stats_df['Dominant Topic'] == 5),\n",
    "    (topic_stats_df['Dominant Topic'] == 6),\n",
    "    (topic_stats_df['Dominant Topic'] == 7),\n",
    "    (topic_stats_df['Dominant Topic'] == 8),\n",
    "    (topic_stats_df['Dominant Topic'] == 9),\n",
    "    (topic_stats_df['Dominant Topic'] == 10),\n",
    "    (topic_stats_df['Dominant Topic'] == 11),\n",
    "    (topic_stats_df['Dominant Topic'] == 12),\n",
    "    (topic_stats_df['Dominant Topic'] == 13),\n",
    "    (topic_stats_df['Dominant Topic'] == 14),\n",
    "    (topic_stats_df['Dominant Topic'] == 15),\n",
    "    (topic_stats_df['Dominant Topic'] == 16),\n",
    "    (topic_stats_df['Dominant Topic'] == 17),\n",
    "    (topic_stats_df['Dominant Topic'] == 18),\n",
    "    (topic_stats_df['Dominant Topic'] == 19),\n",
    "    (topic_stats_df['Dominant Topic'] == 20),\n",
    "    (topic_stats_df['Dominant Topic'] == 21),\n",
    "    (topic_stats_df['Dominant Topic'] == 22),\n",
    "    (topic_stats_df['Dominant Topic'] == 23),\n",
    "    (topic_stats_df['Dominant Topic'] == 24),\n",
    "    (topic_stats_df['Dominant Topic'] == 25),\n",
    "    (topic_stats_df['Dominant Topic'] == 26),\n",
    "    (topic_stats_df['Dominant Topic'] == 27),\n",
    "    (topic_stats_df['Dominant Topic'] == 28),]\n",
    "choices = ['Humanities',\n",
    "'Doctor Relationship',\n",
    "'Summer Experience',\n",
    "'Decision Process',\n",
    "'Service',\n",
    "'Death / Suffering ',\n",
    "'Nursing Home Experience',\n",
    "'Healthcare Access & Equity',\n",
    "'Public Health',\n",
    "'Practice Medicine',\n",
    "'Observe / Shadow',\n",
    "'Anatomy Interest',\n",
    "'Athletics',\n",
    "'Physician Skills',\n",
    "'Impact',\n",
    "'Children / Childhood',\n",
    "'Moments',\n",
    "'Academics',\n",
    "'Culture / Travel',\n",
    "'Emergency Medicine',\n",
    "'Disease / Treatment',\n",
    "'Family Illness',\n",
    "'Rural Community',\n",
    "'Problem Solving',\n",
    "'Become Doctor',\n",
    "'Goals / Tenacity',\n",
    "'Lab Experience',\n",
    "'Biomedical Engineering']\n",
    "topic_stats_df['Dominant Topic Label'] = np.select(conditions, choices)\n",
    "topic_stats_df['Topic Desc'] = [topics_df.iloc[t]['Terms per Topic'] for t in range(len(topic_stats_df))]\n",
    "\n",
    "#Hiding Output for data privacy Reasons \n",
    "#topic_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Representative Documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiding Output for Data Privacy Reasons \n",
    "\n",
    "new_df = corpus_topic_df.groupby('Dominant Topic').apply(lambda topic_set: (topic_set.sort_values(by=['Contribution %'], \n",
    "                                                                                         ascending=False)\n",
    "                                                                             .iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiding Output for Data Privacy Reasons \n",
    "#new_df.to_csv('dominant_essay_topic.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
